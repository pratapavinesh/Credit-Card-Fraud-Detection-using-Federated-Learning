{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_pickle(\"/content/abc.pkl\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPIbfYpq2AcK",
        "outputId": "f3ba4732-49ec-40e5-8aea-643973c848da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      TRANSACTION_ID         TX_DATETIME CUSTOMER_ID TERMINAL_ID  TX_AMOUNT  \\\n",
            "0                  0 2018-04-01 00:00:31         596        3156      57.16   \n",
            "1                  1 2018-04-01 00:02:10        4961        3412      81.51   \n",
            "2                  2 2018-04-01 00:07:56           2        1365     146.00   \n",
            "3                  3 2018-04-01 00:09:29        4128        8737      64.49   \n",
            "4                  4 2018-04-01 00:10:34         927        9906      50.99   \n",
            "...              ...                 ...         ...         ...        ...   \n",
            "9483            9483 2018-04-01 23:56:50        3289        6699      48.69   \n",
            "9484            9484 2018-04-01 23:58:14        3230        6664      85.97   \n",
            "9485            9485 2018-04-01 23:58:31         296        3702     120.88   \n",
            "9486            9486 2018-04-01 23:59:28        2557         146       8.02   \n",
            "9487            9487 2018-04-01 23:59:51         554        8248      19.20   \n",
            "\n",
            "     TX_TIME_SECONDS TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
            "0                 31            0         0                  0  \n",
            "1                130            0         0                  0  \n",
            "2                476            0         0                  0  \n",
            "3                569            0         0                  0  \n",
            "4                634            0         0                  0  \n",
            "...              ...          ...       ...                ...  \n",
            "9483           86210            0         0                  0  \n",
            "9484           86294            0         0                  0  \n",
            "9485           86311            0         0                  0  \n",
            "9486           86368            0         0                  0  \n",
            "9487           86391            0         0                  0  \n",
            "\n",
            "[9488 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pQI_5wLwO3pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to the zip file and the directory where you want to extract it\n",
        "zip_file_path = '/content/data.zip'\n",
        "extract_dir = '/content/data_complete'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Successfully extracted {zip_file_path} to {extract_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHrJMUbD2QoI",
        "outputId": "f68508b5-1d6c-4c23-a685-a9354eb6fd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted /content/data.zip to /content/data_complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# Directory containing the .pkl files\n",
        "directory = '/content/data_complete/data'\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.pkl'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        df = pd.read_pickle(file_path)\n",
        "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
        "\n",
        "excel_file_path = '/content/data_merged.xlsx'\n",
        "\n",
        "# # Convert the DataFrame to a CSV file\n",
        "# merged_df.to_excel(excel_file_path, index=False)\n",
        "chunk_size = 1000000  # 1,000,000 rows per sheet\n",
        "dfs = [merged_df[i:i+chunk_size] for i in range(0, len(merged_df), chunk_size)]\n",
        "\n",
        "# Save each chunk as a separate sheet in the same Excel file\n",
        "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
        "    for idx, chunk_df in enumerate(dfs, start=1):\n",
        "        chunk_df.to_excel(writer, sheet_name=f'Sheet_{idx}', index=False)\n",
        "\n",
        "print(f\"Data saved to {excel_file_path}\")\n",
        "\n",
        "# print(f\"DataFrame saved to {excel_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAwEH3KU4eK_",
        "outputId": "74e7cc2d-624f-41ec-f1ce-bda1d8ba1fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/data_merged.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xlsxwriter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RvsAyiz5bwB",
        "outputId": "717be8c8-0fc9-4761-fd26-fd9ed157794c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64b6-98O8yOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e9ZAHyoB-Gl",
        "outputId": "dfe754d4-b6aa-46cf-ac37-e0caf2857d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   TRANSACTION_ID         TX_DATETIME CUSTOMER_ID TERMINAL_ID  TX_AMOUNT  \\\n",
            "0         1687440 2018-09-24 00:00:27        2524        9558      13.39   \n",
            "1         1687441 2018-09-24 00:01:18        1011        7930      27.35   \n",
            "2         1687442 2018-09-24 00:01:39         872        9471      93.36   \n",
            "3         1687443 2018-09-24 00:02:26        3578        8682     158.79   \n",
            "4         1687444 2018-09-24 00:04:01         942        3757     118.22   \n",
            "\n",
            "  TX_TIME_SECONDS TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
            "0        15206427          176         0                  0  \n",
            "1        15206478          176         0                  0  \n",
            "2        15206499          176         0                  0  \n",
            "3        15206546          176         0                  0  \n",
            "4        15206641          176         0                  0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnLLVoiECWOE",
        "outputId": "d0bea42c-1b6f-4840-a07d-1220e65a4c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSACTION_ID                int64\n",
            "TX_DATETIME          datetime64[ns]\n",
            "CUSTOMER_ID                  object\n",
            "TERMINAL_ID                  object\n",
            "TX_AMOUNT                   float64\n",
            "TX_TIME_SECONDS              object\n",
            "TX_TIME_DAYS                 object\n",
            "TX_FRAUD                      int64\n",
            "TX_FRAUD_SCENARIO             int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCaD5aLxCpLB",
        "outputId": "bc6f8b30-a43e-4cd7-db6b-beae03bd748a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TRANSACTION_ID                    TX_DATETIME     TX_AMOUNT  \\\n",
            "count    1.754155e+06                        1754155  1.754155e+06   \n",
            "mean     8.770770e+05  2018-07-01 11:20:33.708572160  5.363230e+01   \n",
            "min      0.000000e+00            2018-04-01 00:00:31  0.000000e+00   \n",
            "25%      4.385385e+05     2018-05-16 14:40:46.500000  2.101000e+01   \n",
            "50%      8.770770e+05            2018-07-01 11:11:10  4.464000e+01   \n",
            "75%      1.315616e+06     2018-08-16 08:01:01.500000  7.695000e+01   \n",
            "max      1.754154e+06            2018-09-30 23:59:57  2.628000e+03   \n",
            "std      5.063811e+05                            NaN  4.232649e+01   \n",
            "\n",
            "           TX_FRAUD  TX_FRAUD_SCENARIO  \n",
            "count  1.754155e+06       1.754155e+06  \n",
            "mean   8.369272e-03       1.882388e-02  \n",
            "min    0.000000e+00       0.000000e+00  \n",
            "25%    0.000000e+00       0.000000e+00  \n",
            "50%    0.000000e+00       0.000000e+00  \n",
            "75%    0.000000e+00       0.000000e+00  \n",
            "max    1.000000e+00       3.000000e+00  \n",
            "std    9.110012e-02       2.113263e-01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm3nwYXTCs94",
        "outputId": "47719e42-5b60-42de-f6a2-70f20f83db2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSACTION_ID       0\n",
            "TX_DATETIME          0\n",
            "CUSTOMER_ID          0\n",
            "TERMINAL_ID          0\n",
            "TX_AMOUNT            0\n",
            "TX_TIME_SECONDS      0\n",
            "TX_TIME_DAYS         0\n",
            "TX_FRAUD             0\n",
            "TX_FRAUD_SCENARIO    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFYHYmqkCwme",
        "outputId": "168966b8-caf8-4299-9e6a-db95311dc7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRANSACTION_ID       1754155\n",
            "TX_DATETIME          1635076\n",
            "CUSTOMER_ID             4990\n",
            "TERMINAL_ID            10000\n",
            "TX_AMOUNT              24585\n",
            "TX_TIME_SECONDS      1635076\n",
            "TX_TIME_DAYS             183\n",
            "TX_FRAUD                   2\n",
            "TX_FRAUD_SCENARIO          4\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.corr())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtwHITonC-63",
        "outputId": "6ca66bcf-e7f2-4084-b89d-471799de3e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   TRANSACTION_ID  TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  \\\n",
            "TRANSACTION_ID           1.000000     0.999999    -0.000389    -0.000690   \n",
            "TX_DATETIME              0.999999     1.000000    -0.000389    -0.000690   \n",
            "CUSTOMER_ID             -0.000389    -0.000389     1.000000     0.000389   \n",
            "TERMINAL_ID             -0.000690    -0.000690     0.000389     1.000000   \n",
            "TX_AMOUNT                0.000521     0.000521    -0.000757    -0.000166   \n",
            "TX_TIME_SECONDS          0.999999     1.000000    -0.000389    -0.000690   \n",
            "TX_TIME_DAYS             0.999985     0.999992    -0.000389    -0.000690   \n",
            "TX_FRAUD                 0.008689     0.008685    -0.003321    -0.005678   \n",
            "TX_FRAUD_SCENARIO        0.008173     0.008168    -0.004135    -0.004995   \n",
            "\n",
            "                   TX_AMOUNT  TX_TIME_SECONDS  TX_TIME_DAYS  TX_FRAUD  \\\n",
            "TRANSACTION_ID      0.000521         0.999999      0.999985  0.008689   \n",
            "TX_DATETIME         0.000521         1.000000      0.999992  0.008685   \n",
            "CUSTOMER_ID        -0.000757        -0.000389     -0.000389 -0.003321   \n",
            "TERMINAL_ID        -0.000166        -0.000690     -0.000690 -0.005678   \n",
            "TX_AMOUNT           1.000000         0.000521      0.000521  0.168290   \n",
            "TX_TIME_SECONDS     0.000521         1.000000      0.999992  0.008685   \n",
            "TX_TIME_DAYS        0.000521         0.999992      1.000000  0.008687   \n",
            "TX_FRAUD            0.168290         0.008685      0.008687  1.000000   \n",
            "TX_FRAUD_SCENARIO   0.195008         0.008168      0.008169  0.969587   \n",
            "\n",
            "                   TX_FRAUD_SCENARIO  \n",
            "TRANSACTION_ID              0.008173  \n",
            "TX_DATETIME                 0.008168  \n",
            "CUSTOMER_ID                -0.004135  \n",
            "TERMINAL_ID                -0.004995  \n",
            "TX_AMOUNT                   0.195008  \n",
            "TX_TIME_SECONDS             0.008168  \n",
            "TX_TIME_DAYS                0.008169  \n",
            "TX_FRAUD                    0.969587  \n",
            "TX_FRAUD_SCENARIO           1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(merged_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNmfhG54DNCp",
        "outputId": "afa757b2-9fb2-421b-c1ee-69298bd98509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1754155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of rows where TX_FRAUD is 0 or 1\n",
        "fraud_counts = merged_df['TX_FRAUD'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of rows with TX_FRAUD = 0: {fraud_counts.get(0, 0)}\")\n",
        "print(f\"Number of rows with TX_FRAUD = 1: {fraud_counts.get(1, 0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2NH2L9_eys_",
        "outputId": "7d4eac59-79aa-4df7-f00f-b15ef463f893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with TX_FRAUD = 0: 1739474\n",
            "Number of rows with TX_FRAUD = 1: 14681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Convert 'TX_DATETIME' to numerical format (e.g., seconds since epoch)\n",
        "merged_df['TX_DATETIME'] = pd.to_datetime(merged_df['TX_DATETIME'])\n",
        "merged_df['TX_DATETIME'] = merged_df['TX_DATETIME'].astype(int) // 10**9  # Convert to seconds since epoch\n",
        "\n",
        "# Convert 'TX_TIME_SECONDS' and 'TX_TIME_DAYS' to numerical format\n",
        "merged_df['TX_TIME_SECONDS'] = merged_df['TX_TIME_SECONDS'].astype(int)\n",
        "merged_df['TX_TIME_DAYS'] = merged_df['TX_TIME_DAYS'].astype(int)\n",
        "\n",
        "# Define the number of nodes\n",
        "num_nodes = 5\n",
        "\n",
        "# Randomly split the data into training and testing sets for each node\n",
        "dfs_train = []\n",
        "dfs_test = []\n",
        "\n",
        "for i in range(num_nodes):\n",
        "    # Split the data for each node\n",
        "    train_data, test_data = train_test_split(merged_df, test_size=0.2, random_state=i)\n",
        "    dfs_train.append(train_data)\n",
        "    dfs_test.append(test_data)\n",
        "\n",
        "# Initialize global model with default hyperparameters\n",
        "global_model = DecisionTreeClassifier()\n",
        "\n",
        "# Train each node's local model with the same hyperparameters as global model\n",
        "global_predictions=[]\n",
        "for i in range(num_nodes):\n",
        "    # Extract features and target variable for training\n",
        "    X_train = dfs_train[i].drop(['TX_FRAUD', 'TX_FRAUD_SCENARIO', 'TRANSACTION_ID'], axis=1)\n",
        "    y_train = dfs_train[i]['TX_FRAUD']\n",
        "\n",
        "    # Train local model with global model's hyperparameters\n",
        "    local_model = DecisionTreeClassifier(\n",
        "        criterion=global_model.criterion,\n",
        "        splitter=global_model.splitter,\n",
        "        max_depth=global_model.max_depth,\n",
        "        min_samples_split=global_model.min_samples_split,\n",
        "        min_samples_leaf=global_model.min_samples_leaf,\n",
        "        min_weight_fraction_leaf=global_model.min_weight_fraction_leaf,\n",
        "        max_features=global_model.max_features,\n",
        "        random_state=global_model.random_state,\n",
        "        max_leaf_nodes=global_model.max_leaf_nodes,\n",
        "        min_impurity_decrease=global_model.min_impurity_decrease,\n",
        "        class_weight=global_model.class_weight,\n",
        "        ccp_alpha=global_model.ccp_alpha\n",
        "    )\n",
        "\n",
        "    local_model.fit(X_train, y_train)\n",
        "\n",
        "    # Aggregate predictions from local models to form global prediction\n",
        "    x=local_model.predict_proba(dfs_test[i].drop(['TX_FRAUD', 'TX_FRAUD_SCENARIO', 'TRANSACTION_ID'], axis=1))\n",
        "    for _ in x:\n",
        "      global_predictions.append(_)\n",
        "\n",
        "\n",
        "# Combine the aggregated predictions to form the global prediction\n",
        "y_pred_global = np.argmax(global_predictions, axis=1)\n",
        "\n",
        "# Evaluate global model accuracy\n",
        "y_test_global = pd.concat(dfs_test)['TX_FRAUD']\n",
        "global_accuracy = accuracy_score(y_test_global, y_pred_global)\n",
        "print(f\"Global Model Accuracy: {global_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlAxDI34PVIZ",
        "outputId": "412d3bb7-060c-4317-8d3f-c519973f7452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Accuracy: 0.9945033363642324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KL4GF5ktZI_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After creating y_test_global\n",
        "print(\"y_test_global length:\", len(y_test_global))\n",
        "\n",
        "# Right after forming global_predictions\n",
        "print(\"Global predictions shape:\", len(global_predictions))\n",
        "\n",
        "# Comparing the number of predictions with actual labels\n",
        "if len(y_test_global) != len(y_pred_global):\n",
        "    print(\"Mismatch in number of labels and predictions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6VB2NW3WyjK",
        "outputId": "2863d680-2fa8-4272-cd9c-4bcc70cec260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test_global length: 1754155\n",
            "Global predictions shape: 1754155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E68FuRggU5P",
        "outputId": "7c6fee90-e137-4895-ab9b-48518a872de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features and target variable\n",
        "X = merged_df.drop(['TX_FRAUD', 'TX_FRAUD_SCENARIO', 'TRANSACTION_ID'], axis=1)\n",
        "y = merged_df['TX_FRAUD']\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Count the number of rows after applying SMOTE\n",
        "print(f\"Number of rows with TX_FRAUD = 0 after SMOTE: {sum(y_resampled == 0)}\")\n",
        "print(f\"Number of rows with TX_FRAUD = 1 after SMOTE: {sum(y_resampled == 1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-F3S25FgXxq",
        "outputId": "463de7ed-c982-434c-aabd-e0b9cf4c0cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with TX_FRAUD = 0 after SMOTE: 1739474\n",
            "Number of rows with TX_FRAUD = 1 after SMOTE: 1739474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled.reset_index(drop=True, inplace=True)\n",
        "y_resampled.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Combine X and y into a single dataframe\n",
        "merged_df2 = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# Print the first few rows of merged_df2 to verify\n",
        "print(merged_df2.tail())\n",
        "\n",
        "\n",
        "# Count the number of rows where TX_FRAUD is 0 or 1\n",
        "fraud_counts = merged_df2['TX_FRAUD'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of rows with TX_FRAUD = 0: {fraud_counts.get(0, 0)}\")\n",
        "print(f\"Number of rows with TX_FRAUD = 1: {fraud_counts.get(1, 0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyoqy2ushQi4",
        "outputId": "c660ee72-a3c6-4c15-e3c2-535d7d2d4b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         TX_DATETIME  CUSTOMER_ID  TERMINAL_ID   TX_AMOUNT  TX_TIME_SECONDS  \\\n",
            "3478943            0   1573.35121  5168.719307   39.396296          2669730   \n",
            "3478944            0  4180.716724  1425.847857  171.827175         11961944   \n",
            "3478945            0  4315.670751  9767.841156   87.075590           833338   \n",
            "3478946            0  1952.483705  9324.230538  307.112897          2465055   \n",
            "3478947            0  1937.505339  4123.258694   84.714086          9128840   \n",
            "\n",
            "         TX_TIME_DAYS  TX_FRAUD  \n",
            "3478943            30         1  \n",
            "3478944           138         1  \n",
            "3478945             9         1  \n",
            "3478946            28         1  \n",
            "3478947           105         1  \n",
            "Number of rows with TX_FRAUD = 0: 1739474\n",
            "Number of rows with TX_FRAUD = 1: 1739474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Convert 'TX_DATETIME' to numerical format (e.g., seconds since epoch)\n",
        "merged_df2['TX_DATETIME'] = pd.to_datetime(merged_df2['TX_DATETIME'])\n",
        "merged_df2['TX_DATETIME'] = merged_df2['TX_DATETIME'].astype(int) // 10**9  # Convert to seconds since epoch\n",
        "\n",
        "# Convert 'TX_TIME_SECONDS' and 'TX_TIME_DAYS' to numerical format\n",
        "merged_df2['TX_TIME_SECONDS'] = merged_df2['TX_TIME_SECONDS'].astype(int)\n",
        "merged_df2['TX_TIME_DAYS'] = merged_df2['TX_TIME_DAYS'].astype(int)\n",
        "\n",
        "# Define the number of nodes\n",
        "num_nodes = 5\n",
        "\n",
        "# Randomly split the data into training and testing sets for each node\n",
        "dfs_train = []\n",
        "dfs_test = []\n",
        "\n",
        "for i in range(num_nodes):\n",
        "    # Split the data for each node\n",
        "    train_data, test_data = train_test_split(merged_df2, test_size=0.2, random_state=i)\n",
        "    dfs_train.append(train_data)\n",
        "    dfs_test.append(test_data)\n",
        "\n",
        "# Initialize global model with default hyperparameters\n",
        "global_model = DecisionTreeClassifier()\n",
        "\n",
        "# Train each node's local model with the same hyperparameters as global model\n",
        "global_predictions=[]\n",
        "for i in range(num_nodes):\n",
        "    # Extract features and target variable for training\n",
        "    X_train = dfs_train[i].drop(['TX_FRAUD'], axis=1)\n",
        "    y_train = dfs_train[i]['TX_FRAUD']\n",
        "\n",
        "    # Train local model with global model's hyperparameters\n",
        "    local_model = DecisionTreeClassifier(\n",
        "        criterion=global_model.criterion,\n",
        "        splitter=global_model.splitter,\n",
        "        max_depth=global_model.max_depth,\n",
        "        min_samples_split=global_model.min_samples_split,\n",
        "        min_samples_leaf=global_model.min_samples_leaf,\n",
        "        min_weight_fraction_leaf=global_model.min_weight_fraction_leaf,\n",
        "        max_features=global_model.max_features,\n",
        "        random_state=global_model.random_state,\n",
        "        max_leaf_nodes=global_model.max_leaf_nodes,\n",
        "        min_impurity_decrease=global_model.min_impurity_decrease,\n",
        "        class_weight=global_model.class_weight,\n",
        "        ccp_alpha=global_model.ccp_alpha\n",
        "    )\n",
        "\n",
        "    local_model.fit(X_train, y_train)\n",
        "\n",
        "    # Aggregate predictions from local models to form global prediction\n",
        "    x=local_model.predict_proba(dfs_test[i].drop(['TX_FRAUD'], axis=1))\n",
        "    for _ in x:\n",
        "      global_predictions.append(_)\n",
        "\n",
        "\n",
        "# Combine the aggregated predictions to form the global prediction\n",
        "y_pred_global = np.argmax(global_predictions, axis=1)\n",
        "\n",
        "# Evaluate global model accuracy\n",
        "y_test_global = pd.concat(dfs_test)['TX_FRAUD']\n",
        "global_accuracy = accuracy_score(y_test_global, y_pred_global)\n",
        "print(f\"Global Model Accuracy: {global_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QneW_0vgbvo",
        "outputId": "75e5e241-56e8-4f3f-a84e-8aea9d65a854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Accuracy: 0.8165055877491773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_global, y_pred_global)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(y_test_global, y_pred_global)\n",
        "recall = recall_score(y_test_global, y_pred_global)\n",
        "f1 = f1_score(y_test_global, y_pred_global)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Evaluate global model accuracy\n",
        "global_accuracy = accuracy_score(y_test_global, y_pred_global)\n",
        "print(f\"Global Model Accuracy: {global_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIlBrkcGmswB",
        "outputId": "d0ea0a01-7b00-4ad4-b565-be3f59183c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[263258  84528]\n",
            " [ 43122 304882]]\n",
            "Precision: 0.7829\n",
            "Recall: 0.8761\n",
            "F1 Score: 0.8269\n",
            "Global Model Accuracy: 0.8165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DL"
      ],
      "metadata": {
        "id": "0t4WGNySlU-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = merged_df2.drop('TX_FRAUD', axis=1)\n",
        "y = merged_df2['TX_FRAUD']\n",
        "\n",
        "# Number of nodes\n",
        "num_nodes = 5\n",
        "\n",
        "# Initialize global MLP model\n",
        "global_model = MLPClassifier(random_state=42)\n",
        "\n",
        "# Lists to store local models and predictions\n",
        "local_models = []\n",
        "local_X_tests = []\n",
        "local_y_tests = []\n",
        "\n",
        "# Split the dataset into num_nodes subsets and train local models\n",
        "subset_size = len(X) // num_nodes\n",
        "\n",
        "for i in range(num_nodes):\n",
        "    # Split the data into training and testing subsets\n",
        "    start_idx = i * subset_size\n",
        "    end_idx = start_idx + subset_size\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X[start_idx:end_idx],\n",
        "                                                        y[start_idx:end_idx],\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "\n",
        "    # Initialize and train local MLP model\n",
        "    local_model = MLPClassifier(random_state=42)\n",
        "    local_model.fit(X_train, y_train)\n",
        "\n",
        "    # Store local model\n",
        "    local_models.append(local_model)\n",
        "    local_X_tests.append(X_test)\n",
        "    local_y_tests.append(y_test)\n",
        "\n",
        "# Make predictions on the global dataset using local models\n",
        "global_predictions = []\n",
        "\n",
        "for i, local_model in enumerate(local_models):\n",
        "    y_pred_global_part = local_model.predict(local_X_tests[i])\n",
        "    global_predictions.extend(y_pred_global_part)\n",
        "\n",
        "# Evaluate global model accuracy\n",
        "global_accuracy = accuracy_score(np.concatenate(local_y_tests), global_predictions)\n",
        "print(f\"Global Model Accuracy: {global_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vzN0ehk3Kzi",
        "outputId": "da20b3a9-1b99-44a8-9b5e-c78cf74d8471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Accuracy: 0.8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(np.concatenate(local_y_tests), global_predictions)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = precision_score(np.concatenate(local_y_tests), global_predictions)\n",
        "recall = recall_score(np.concatenate(local_y_tests), global_predictions)\n",
        "f1 = f1_score(np.concatenate(local_y_tests), global_predictions)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Evaluate global model accuracy\n",
        "global_accuracy = accuracy_score(np.concatenate(local_y_tests), global_predictions)\n",
        "print(f\"Global Model Accuracy: {global_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRq8o2s19DnE",
        "outputId": "7f55d499-6999-49b8-a842-c2bda258da05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[300204  47582]\n",
            " [ 30753 317251]]\n",
            "Precision: 0.8696\n",
            "Recall: 0.9116\n",
            "F1 Score: 0.8901\n",
            "Global Model Accuracy: 0.8874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tR0wY_maEPUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}